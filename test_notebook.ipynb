{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.jit as jit\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple\n",
    "from torch import Tensor\n",
    "import numbers\n",
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import copy\n",
    "from scipy.integrate import quad\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_message = pd.read_csv('data/clean/AMZN_message_data.csv',index_col=0)\n",
    "df_orders = pd.read_csv('data/clean/AMZN_orderbook_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ask price 1</th>\n",
       "      <th>ask size 1</th>\n",
       "      <th>bid price 1</th>\n",
       "      <th>bid size 1</th>\n",
       "      <th>ask price 2</th>\n",
       "      <th>ask size 2</th>\n",
       "      <th>bid price 2</th>\n",
       "      <th>bid size 2</th>\n",
       "      <th>ask price 3</th>\n",
       "      <th>...</th>\n",
       "      <th>bid size 3</th>\n",
       "      <th>ask price 4</th>\n",
       "      <th>ask size 4</th>\n",
       "      <th>bid price 4</th>\n",
       "      <th>bid size 4</th>\n",
       "      <th>ask price 5</th>\n",
       "      <th>ask size 5</th>\n",
       "      <th>bid price 5</th>\n",
       "      <th>bid size 5</th>\n",
       "      <th>state indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239900</td>\n",
       "      <td>100</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2240000</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>2242500</td>\n",
       "      <td>100</td>\n",
       "      <td>2230400</td>\n",
       "      <td>100</td>\n",
       "      <td>2244000</td>\n",
       "      <td>547</td>\n",
       "      <td>2230000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>2240000</td>\n",
       "      <td>220</td>\n",
       "      <td>2230400</td>\n",
       "      <td>100</td>\n",
       "      <td>2242500</td>\n",
       "      <td>100</td>\n",
       "      <td>2230000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2237500</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>2240000</td>\n",
       "      <td>220</td>\n",
       "      <td>2230700</td>\n",
       "      <td>200</td>\n",
       "      <td>2242500</td>\n",
       "      <td>100</td>\n",
       "      <td>2230400</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2237500</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>2240000</td>\n",
       "      <td>233</td>\n",
       "      <td>2230700</td>\n",
       "      <td>200</td>\n",
       "      <td>2242500</td>\n",
       "      <td>100</td>\n",
       "      <td>2230400</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2239500</td>\n",
       "      <td>100</td>\n",
       "      <td>2238100</td>\n",
       "      <td>21</td>\n",
       "      <td>2239600</td>\n",
       "      <td>20</td>\n",
       "      <td>2237500</td>\n",
       "      <td>100</td>\n",
       "      <td>2239900</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2240000</td>\n",
       "      <td>233</td>\n",
       "      <td>2231800</td>\n",
       "      <td>100</td>\n",
       "      <td>2242500</td>\n",
       "      <td>100</td>\n",
       "      <td>2230700</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144061</th>\n",
       "      <td>144061</td>\n",
       "      <td>2206200</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>309</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206500</td>\n",
       "      <td>...</td>\n",
       "      <td>700</td>\n",
       "      <td>2206700</td>\n",
       "      <td>170</td>\n",
       "      <td>2204700</td>\n",
       "      <td>100</td>\n",
       "      <td>2206900</td>\n",
       "      <td>170</td>\n",
       "      <td>2204600</td>\n",
       "      <td>1704</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144062</th>\n",
       "      <td>144062</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>...</td>\n",
       "      <td>700</td>\n",
       "      <td>2206900</td>\n",
       "      <td>170</td>\n",
       "      <td>2204700</td>\n",
       "      <td>100</td>\n",
       "      <td>2207100</td>\n",
       "      <td>1800</td>\n",
       "      <td>2204600</td>\n",
       "      <td>1704</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144063</th>\n",
       "      <td>144063</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>...</td>\n",
       "      <td>700</td>\n",
       "      <td>2207100</td>\n",
       "      <td>1800</td>\n",
       "      <td>2204700</td>\n",
       "      <td>100</td>\n",
       "      <td>2207400</td>\n",
       "      <td>800</td>\n",
       "      <td>2204600</td>\n",
       "      <td>1704</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144064</th>\n",
       "      <td>144064</td>\n",
       "      <td>2206300</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206500</td>\n",
       "      <td>...</td>\n",
       "      <td>700</td>\n",
       "      <td>2206700</td>\n",
       "      <td>170</td>\n",
       "      <td>2204700</td>\n",
       "      <td>100</td>\n",
       "      <td>2207100</td>\n",
       "      <td>1800</td>\n",
       "      <td>2204600</td>\n",
       "      <td>1704</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144065</th>\n",
       "      <td>144065</td>\n",
       "      <td>2206400</td>\n",
       "      <td>100</td>\n",
       "      <td>2205100</td>\n",
       "      <td>249</td>\n",
       "      <td>2206500</td>\n",
       "      <td>1290</td>\n",
       "      <td>2205000</td>\n",
       "      <td>71</td>\n",
       "      <td>2206700</td>\n",
       "      <td>...</td>\n",
       "      <td>700</td>\n",
       "      <td>2207100</td>\n",
       "      <td>1800</td>\n",
       "      <td>2204700</td>\n",
       "      <td>100</td>\n",
       "      <td>2207400</td>\n",
       "      <td>800</td>\n",
       "      <td>2204600</td>\n",
       "      <td>1704</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144066 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  ask price 1  ask size 1  bid price 1  bid size 1  \\\n",
       "0                0      2239500         100      2238100          21   \n",
       "1                1      2239500         100      2238100          21   \n",
       "2                2      2239500         100      2238100          21   \n",
       "3                3      2239500         100      2238100          21   \n",
       "4                4      2239500         100      2238100          21   \n",
       "...            ...          ...         ...          ...         ...   \n",
       "144061      144061      2206200         100      2205100         309   \n",
       "144062      144062      2206400         100      2205100         249   \n",
       "144063      144063      2206400         100      2205100         249   \n",
       "144064      144064      2206300         100      2205100         249   \n",
       "144065      144065      2206400         100      2205100         249   \n",
       "\n",
       "        ask price 2  ask size 2  bid price 2  bid size 2  ask price 3  ...  \\\n",
       "0           2239900         100      2231800         100      2240000  ...   \n",
       "1           2239600          20      2231800         100      2239900  ...   \n",
       "2           2239600          20      2237500         100      2239900  ...   \n",
       "3           2239600          20      2237500         100      2239900  ...   \n",
       "4           2239600          20      2237500         100      2239900  ...   \n",
       "...             ...         ...          ...         ...          ...  ...   \n",
       "144061      2206400         100      2205000          71      2206500  ...   \n",
       "144062      2206500        1290      2205000          71      2206700  ...   \n",
       "144063      2206500        1290      2205000          71      2206700  ...   \n",
       "144064      2206400         100      2205000          71      2206500  ...   \n",
       "144065      2206500        1290      2205000          71      2206700  ...   \n",
       "\n",
       "        bid size 3  ask price 4  ask size 4  bid price 4  bid size 4  \\\n",
       "0              200      2242500         100      2230400         100   \n",
       "1              200      2240000         220      2230400         100   \n",
       "2              100      2240000         220      2230700         200   \n",
       "3              100      2240000         233      2230700         200   \n",
       "4                2      2240000         233      2231800         100   \n",
       "...            ...          ...         ...          ...         ...   \n",
       "144061         700      2206700         170      2204700         100   \n",
       "144062         700      2206900         170      2204700         100   \n",
       "144063         700      2207100        1800      2204700         100   \n",
       "144064         700      2206700         170      2204700         100   \n",
       "144065         700      2207100        1800      2204700         100   \n",
       "\n",
       "        ask price 5  ask size 5  bid price 5  bid size 5  state indicator  \n",
       "0           2244000         547      2230000          10              0.0  \n",
       "1           2242500         100      2230000          10              0.0  \n",
       "2           2242500         100      2230400         100              0.0  \n",
       "3           2242500         100      2230400         100              0.0  \n",
       "4           2242500         100      2230700         200              0.0  \n",
       "...             ...         ...          ...         ...              ...  \n",
       "144061      2206900         170      2204600        1704              2.0  \n",
       "144062      2207100        1800      2204600        1704              2.0  \n",
       "144063      2207400         800      2204600        1704              2.0  \n",
       "144064      2207100        1800      2204600        1704              2.0  \n",
       "144065      2207400         800      2204600        1704              2.0  \n",
       "\n",
       "[144066 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_raw = df_message['time'].to_numpy()\n",
    "events_raw = df_message['event type thesis'].to_numpy()\n",
    "states_raw = df_orders['state indicator'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_rolling_window(arr, window_len):\n",
    "    return sliding_window_view(arr, window_shape = window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time = split_rolling_window(time_raw, window_len)\n",
    "events = split_rolling_window(events_raw, window_len)\n",
    "states = split_rolling_window(states_raw, window_len)\n",
    "trans_mtrx = np.load('data/transition_matrices/AMZN_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayerStacked(nn.Module):\n",
    "    def __init__(self, cell, input_size, hidden_size, t):\n",
    "        super().__init__()\n",
    "        self.stackedcell = LSTMStackedCells(cell, input_size, hidden_size)\n",
    "        self.num_layers = 4\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_layer = Embed_layer(input_size, hidden_size)\n",
    "        self.h_input_merge_LC = Hidden_sharing_layer(5*hidden_size, hidden_size)\n",
    "        self.intensity_functions = []\n",
    "        self.t_seq = t\n",
    "        \n",
    "    def forward(self, inputs, t_seq):\n",
    "        first_run = True\n",
    "        \n",
    "        h1, h2, h3, h4 = [Variable(torch.randn(self.hidden_size)) for i in range(4)]\n",
    "        c1_1, c2_1, c3_1, c4_1 = [Variable(torch.randn(self.hidden_size)) for i in range(4)]\n",
    "        c1_2, c2_2, c3_2, c4_2 = [Variable(torch.randn(self.hidden_size)) for i in range(4)]\n",
    "        \n",
    "        for input, tj in zip(inputs, self.t_seq):\n",
    "            embedded_input = self.embed_layer(input)\n",
    "            \n",
    "            if not first_run: #TODO bit messy way to initialise run & set cell states. Clean it up if time left\n",
    "                c1_1, c2_1, c3_1, c4_1 = self.fill_in_func(c1_func_list, tj)\n",
    "                h1, h2, h3, h4 = self.fill_in_func(h_func_list, tj)\n",
    "                c1_2, c2_2, c3_2, c4_2 = c2_list\n",
    "                \n",
    "            h1, h2, h3, h4 = self.process_hidden_states_and_input(embedded_input, h1, h2, h3, h4)\n",
    "            \n",
    "            state = [(h1, c1_1, c1_2), (h2, c2_1, c2_2), (h3, c3_1, c3_2), (h4, c4_1, c4_2)]\n",
    "            \n",
    "            c2_list, c1_func_list, h_func_list, intensity_functions = self.stackedcell(state, tj)\n",
    "            \n",
    "            self.intensity_functions.append(intensity_functions)\n",
    "            \n",
    "            first_run = False\n",
    "            \n",
    "        return self.intensity_functions\n",
    "            \n",
    "    def process_hidden_states_and_input(self, embedded_input, h1, h2, h3, h4):\n",
    "        h_merged_with_input =  self.h_input_merge_LC(torch.concat([embedded_input, h1, h2, h3, h4]))\n",
    "        h1, h2, h3, h4 = torch.split(h_merged_with_input, self.hidden_size)\n",
    "        return h1, h2, h3, h4\n",
    "    \n",
    "            \n",
    "    def fill_in_func(self, func, tj):\n",
    "        f1, f2, f3, f4 = func[0](tj), func[1](tj), func[2](tj), func[3](tj)\n",
    "        return (f1, f2, f3, f4)\n",
    "    \n",
    "class LSTMStackedCells(nn.Module):\n",
    "    def __init__(self, cell, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.cell1 = cell(hidden_size, hidden_size)\n",
    "        self.cell2 = cell(hidden_size, hidden_size)\n",
    "        self.cell3 = cell(hidden_size, hidden_size)\n",
    "        self.cell4 = cell(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, state, tj):\n",
    "        state1, state2, state3, state4 = state\n",
    "        \n",
    "        c1_2, c1_1_func, h1_func = self.cell1(state1, tj)\n",
    "        c2_2, c2_1_func, h2_func = self.cell2(state2, tj)\n",
    "        c3_2, c3_1_func, h3_func = self.cell3(state3, tj)\n",
    "        c4_2, c4_1_func, h4_func = self.cell4(state4, tj)\n",
    "        \n",
    "        intensity_functions = self.get_intensity_functions()\n",
    "        c2_list = [c1_2, c2_2, c3_2, c4_2]\n",
    "        c1_func_list = [c1_1_func, c2_1_func, c3_1_func, c4_1_func]\n",
    "        h_func_list = [h1_func, h2_func, h3_func, h4_func]\n",
    "        \n",
    "        return c2_list, c1_func_list, h_func_list, intensity_functions\n",
    "\n",
    "    def get_intensity_functions(self): #using copies since the constants in object used for f() will be changing through time. TODO: add function and variable to a seperate dataclass that can be saved and isn't as clunky.\n",
    "        f1 = self.cell1.get_frozen_intensity_function()\n",
    "        f2 = self.cell2.get_frozen_intensity_function()\n",
    "        f3 = self.cell3.get_frozen_intensity_function()\n",
    "        f4 = self.cell4.get_frozen_intensity_function()\n",
    "        return [f1, f2, f3, f4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.LC_i1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_i2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_o = Gate_network(input_size, hidden_size)\n",
    "        self.LC_z = Gate_network(input_size, hidden_size)\n",
    "        self.LC_d = Decay_network(input_size, hidden_size)\n",
    "        self.LC_lambda = Decode_layer(input_size, hidden_size)\n",
    "        \n",
    "        self.tj = None\n",
    "        self.cy1 = None\n",
    "        self.cy2 = None\n",
    "        self.decay_coef = None\n",
    "        self.o = None\n",
    "        \n",
    "    def get_frozen_intensity_function(self): #variables are put in data_class so they become constants.\n",
    "        intensity_dataclass = Intensity_func_and_constants(self.tj, self.cy1, self.cy2, self.decay_coef, self.o, self.LC_lambda)\n",
    "        return intensity_dataclass.get_intensity_function()\n",
    "        \n",
    "    def get_intensity(self, t):\n",
    "        h_t = self.get_h(t)\n",
    "        intensity = self.LC_lambda(h_t)\n",
    "        return intensity\n",
    "        \n",
    "    def get_h(self, t):\n",
    "        c_t = self.get_c(t)\n",
    "        h_t = self.o * torch.tanh(c_t)\n",
    "        return h_t\n",
    "\n",
    "    def get_c(self, t):\n",
    "        c_t = self.cy2 + (self.cy1 - self.cy2)*torch.exp(-self.decay_coef*(t - self.tj))\n",
    "        return c_t\n",
    "        \n",
    "    def forward(self, state, tj):\n",
    "        self.tj = tj\n",
    "        \n",
    "        hx, cx1, cx2 = state\n",
    "        \n",
    "        i1 = self.LC_i1(hx)\n",
    "        i2 = self.LC_i2(hx)\n",
    "        f1 = self.LC_f1(hx)\n",
    "        f2 = self.LC_f2(hx)\n",
    "        o = self.LC_o(hx)\n",
    "        z = self.Tanh(self.LC_z(hx))\n",
    "\n",
    "        self.o = o\n",
    "        self.cy1 = (f1 * cx1) + (i1 * z)\n",
    "        self.cy2 = (f2 * cx2) + (i2 * z)\n",
    "        self.decay_coef = self.LC_d(hx)\n",
    "        \n",
    "        return self.cy2, self.get_c, self.get_h\n",
    "    \n",
    "class Intensity_func_and_constants():\n",
    "    def __init__(self, tj, c1, c2, decay_coeff, o, LC_lambda):\n",
    "        self.tj = tj\n",
    "        self.cy1 = c1\n",
    "        self.cy2 = c2\n",
    "        self.decay_coef = decay_coeff       \n",
    "        self.o = o\n",
    "        self.LC_lambda = LC_lambda\n",
    "            \n",
    "    def get_intensity_function(self):\n",
    "        return self.get_intensity\n",
    "            \n",
    "    def get_intensity(self, t):\n",
    "        h_t = self.get_h(t)\n",
    "        intensity = self.LC_lambda(h_t)\n",
    "        return intensity\n",
    "        \n",
    "    def get_h(self, t):\n",
    "        c_t = self.get_c(t)\n",
    "        h_t = self.o * torch.tanh(c_t)\n",
    "        return h_t\n",
    "\n",
    "    def get_c(self, t):\n",
    "        c_t = self.cy2 + (self.cy1 - self.cy2)*torch.exp(-self.decay_coef*(t - self.tj))\n",
    "        return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate_network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.tanh(self.layer1(input))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        return self.Sigmoid(x)\n",
    "    \n",
    "class Decay_network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.softplus(self.layer1(input))\n",
    "        x = self.softplus(self.layer2(x))\n",
    "        return x\n",
    "    \n",
    "class Decode_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.softplus(self.layer1(input))\n",
    "        x = self.softplus(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "class Embed_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.tanh(self.layer1(input))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        return x\n",
    "    \n",
    "class Hidden_sharing_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, 4*hidden_size)\n",
    "        self.layer2 = nn.Linear(4*hidden_size, 4*hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.tanh(self.layer1(input))\n",
    "        x = self.tanh(self.layer2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMLayerStacked(LSTMCell, 2, 16, time[0])\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: -0.06021554183959961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\autograd\\__init__.py:173: UserWarning: Error detected in MmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 461, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 450, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 652, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2768, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2814, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3012, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3251, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\2456142687.py\", line 1, in <module>\n",
      "    train_one_epoch(torch.randn(len(time), window_len, 2)) #error happened after moving time from initialization to forward.\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\2145435500.py\", line 17, in train_one_epoch\n",
      "    intensity_funcs = model(inputs, time[i])\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\2919157019.py\", line 31, in forward\n",
      "    c2_list, c1_func_list, h_func_list, intensity_functions = self.stackedcell(state, tj)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\2919157019.py\", line 60, in forward\n",
      "    c1_2, c1_1_func, h1_func = self.cell1(state1, tj)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\3924886943.py\", line 55, in forward\n",
      "    self.decay_coef = self.LC_d(hx)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\AppData\\Local\\Temp\\ipykernel_18852\\2430483777.py\", line 25, in forward\n",
      "    x = self.softplus(self.layer2(x))\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\jobei\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:104.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 16]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\machinelearning\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 16]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "train_one_epoch(torch.randn(len(time), window_len, 2)) #error happened after moving time from initialization to forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(data):\n",
    "    \n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, inputs in enumerate(data):\n",
    "        \n",
    "        \n",
    "        # Every data instance is an input + label pair\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        intensity_funcs = model(inputs, time[i])\n",
    "        loss_fn = HawkesLoss(intensity_funcs, time[i], events[i], trans_mtrx, states[i])\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn()\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1 == 0:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HawkesLoss(nn.Module):\n",
    "    def __init__(self, intensity_functions, t_seq, event_seq, trans_mtrx, state_seq):\n",
    "        super().__init__()\n",
    "        self.intensity_functions = intensity_functions\n",
    "        self.t_seq = t_seq\n",
    "        self.event_seq = event_seq\n",
    "        self.trans_mtrx = trans_mtrx\n",
    "        self.state_seq = state_seq\n",
    "        self.t_intervals = self.get_time_intervals()\n",
    "        self.n1, self.n2 = 1, 1\n",
    "        self.n_k = 4\n",
    "        \n",
    "    def get_time_intervals(self):\n",
    "        return np.hstack([self.t_seq[:-1, np.newaxis], self.t_seq[1:, np.newaxis]])\n",
    "\n",
    "    def forward(self):\n",
    "        n1, n2 = np.random.randn(2)    #FOR NOW RANDOM WEIGHTING COEFFICIENT. MAYBE MAKE DETERMINISTIC LATER\n",
    "        L1 = self.get_L1()\n",
    "        L2 = self.get_L2()\n",
    "        L3 = self.get_L3()\n",
    "        hawkes_loss = L1 - self.n1*L2 + self.n2*L3 \n",
    "        return hawkes_loss\n",
    "\n",
    "    def get_L1(self):\n",
    "        sum_L1 = 0\n",
    "        for i, (t_begin, t_end) in enumerate(self.t_intervals):\n",
    "            k = self.event_seq[i]\n",
    "            intensity_new_k = self.intensity_functions[i][k]\n",
    "            old_intensity_functions = self.intensity_functions[i - 1]\n",
    "            total_intensity_integral = self.get_summed_intensity_integral(old_intensity_functions, t_begin, t_end)\n",
    "            sum_L1 += torch.log(intensity_new_k(t_end)) - total_intensity_integral\n",
    "        return sum_L1\n",
    "\n",
    "    def get_summed_intensity_integral(self, intensity_functions, t_begin, t_end):\n",
    "        total_integral = 0\n",
    "        for function in intensity_functions:\n",
    "            total_integral += quad(function, t_begin, t_end)[0]\n",
    "        return total_integral\n",
    "\n",
    "    def get_L2(self):\n",
    "        sum_L2 = 0\n",
    "        for i in range(len(self.event_seq) - 1):\n",
    "            event = self.event_seq[i]\n",
    "            tjplus1 = self.t_seq[i+1]\n",
    "            event_intensity_function = self.intensity_functions[i][event]\n",
    "            intesity_event = event_intensity_function(tjplus1)\n",
    "            sum_L2 += -torch.log(intesity_event)\n",
    "        return sum_L2\n",
    "\n",
    "    def get_L3(self):\n",
    "        sum_L3 = 0\n",
    "        for i in range(0, len(self.event_seq) - 1):\n",
    "            yiplus1 = int(self.event_seq[i + 1])\n",
    "            xi = int(self.state_seq[i])\n",
    "            xiplus1 = int(self.state_seq[i + 1])\n",
    "            transition_prob = self.trans_mtrx[yiplus1][xi][xiplus1]\n",
    "            sum_L3 += np.log(transition_prob)\n",
    "        return sum_L3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250187801"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(low=0, high= 2**32,dtype='uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "001b6d5e8c8d1c70c2bf73c4b362256a5cb28fdcf5b48d35800dbe28f3238531"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
