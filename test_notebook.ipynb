{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.jit as jit\n",
    "import warnings\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple\n",
    "from torch import Tensor\n",
    "import numbers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.n_gates = 6\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_ih = Parameter(torch.randn(self.n_gates * hidden_size, input_size))\n",
    "        self.weight_hh = Parameter(torch.randn(self.n_gates * hidden_size, hidden_size))\n",
    "        self.bias_ih = Parameter(torch.randn(self.n_gates * hidden_size))\n",
    "        self.bias_hh = Parameter(torch.randn(self.n_gates * hidden_size))\n",
    "\n",
    "    def forward(self, input: Tensor, state: Tuple[Tensor, Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        hx, cx1, cx2 = state\n",
    "        \n",
    "        gates = (torch.matmul(input, self.weight_ih.t()) + self.bias_ih +\n",
    "                 torch.matmul(hx, self.weight_hh.t()) + self.bias_hh)\n",
    "        \n",
    "        ingate1, ingate2, forgetgate1, forgetgate2, cellgate, outgate = gates.chunk(self.n_gates)\n",
    "\n",
    "        ingate1 = torch.sigmoid(ingate1)\n",
    "        ingate2 = torch.sigmoid(ingate2)\n",
    "        forgetgate1 = torch.sigmoid(forgetgate1)\n",
    "        forgetgate2 = torch.sigmoid(forgetgate2)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "        \n",
    "\n",
    "        cy1 = (forgetgate1 * cx1) + (ingate1 * cellgate)\n",
    "        cy2 = (forgetgate2 * cx2) + (ingate2 * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy1, cy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell1.weight_ih tensor([[ 1.0475, -1.2576,  1.4938, -0.1403],\n",
      "        [-0.1761,  0.2446, -0.0066,  0.0169],\n",
      "        [ 0.6526, -0.3397, -0.0401,  0.2991],\n",
      "        [-0.8911, -0.1530,  0.4985,  0.1815],\n",
      "        [ 0.8046, -0.3014, -0.1771,  0.5974],\n",
      "        [ 1.9203, -2.3702,  1.0451, -1.3581],\n",
      "        [-0.5414,  0.9392,  0.4592,  0.1241],\n",
      "        [ 0.5214, -0.8826,  2.0588, -0.1296],\n",
      "        [ 0.9739,  0.3298, -1.9964,  1.2978],\n",
      "        [ 0.6754, -0.5145, -1.5474, -0.3484],\n",
      "        [ 0.2155, -0.7136, -0.7659,  0.3172],\n",
      "        [ 0.9694,  1.2144,  1.5976,  0.9273],\n",
      "        [-0.0898,  0.1910,  2.2066,  1.9341],\n",
      "        [ 2.2889, -1.9971,  1.0205,  1.2434],\n",
      "        [ 0.0137,  0.4146, -0.0572, -1.7984],\n",
      "        [ 0.5057, -1.2772, -1.9820,  0.5453],\n",
      "        [-0.4307, -1.6967, -0.6890,  0.2548],\n",
      "        [ 0.6678,  0.2351, -0.7527,  0.1685]])\n",
      "cell1.weight_hh tensor([[ 1.2539, -0.3586, -1.7616],\n",
      "        [-0.9085,  0.3503,  0.1402],\n",
      "        [ 0.7802,  0.1413,  0.2270],\n",
      "        [ 1.4114, -0.4574,  0.1459],\n",
      "        [ 1.7407,  0.2840, -1.0095],\n",
      "        [-1.0328,  0.9711, -1.3253],\n",
      "        [-0.6064, -0.1141, -0.8586],\n",
      "        [ 1.1369,  0.5676, -0.2033],\n",
      "        [-2.0506,  0.4777, -0.6397],\n",
      "        [-0.0571,  1.0347, -0.2541],\n",
      "        [ 1.5730, -1.1796,  0.6499],\n",
      "        [-1.0614,  0.1236,  0.6280],\n",
      "        [ 0.1998, -0.3456, -0.0459],\n",
      "        [ 0.4678,  1.0547, -0.6242],\n",
      "        [ 0.3321,  0.0827,  0.3313],\n",
      "        [-0.6791,  0.0155,  2.8240],\n",
      "        [ 2.2163, -2.5000,  0.8705],\n",
      "        [-0.5052,  1.2613, -0.2773]])\n",
      "cell1.bias_ih tensor([ 0.3167, -0.4491, -0.9128, -0.7955,  0.2864, -0.1710, -0.3050,  0.5377,\n",
      "         1.5283,  1.3364,  1.5320, -2.7381,  0.3528, -0.3815, -0.9309,  0.4054,\n",
      "         0.4928, -0.7538])\n",
      "cell1.bias_hh tensor([ 0.1555,  1.5746, -2.4735,  0.9388, -2.2034, -2.3861, -0.6623,  0.1386,\n",
      "         1.3120, -0.5256,  1.3973,  0.8893, -1.4888, -0.6909,  0.3993, -0.7310,\n",
      "        -0.1849,  0.4785])\n",
      "cell2.weight_ih tensor([[-0.9019, -0.1405, -1.0168,  2.0125],\n",
      "        [-0.6461,  0.4990, -0.7485,  0.2999],\n",
      "        [-0.6183, -1.1121, -0.9652,  1.8302],\n",
      "        [ 0.8927, -0.0186, -0.9969, -1.0495],\n",
      "        [ 1.7284, -0.8360, -0.5098, -0.8361],\n",
      "        [-0.1245, -0.0065, -0.7985, -0.6487],\n",
      "        [ 1.8217, -1.3388, -1.9191, -0.3601],\n",
      "        [ 0.3064, -0.2313,  0.0879, -0.2797],\n",
      "        [-1.1837,  2.0129,  2.3775,  1.2906],\n",
      "        [-0.6131, -0.5320, -0.5620, -0.1554],\n",
      "        [-1.2304,  0.0632,  0.9729, -1.1269],\n",
      "        [ 0.0318, -0.4766, -1.2147,  0.3214],\n",
      "        [ 0.7814, -0.1207, -1.8325, -0.1579],\n",
      "        [ 1.0119,  2.1483, -0.2305,  2.4035],\n",
      "        [-0.8392,  0.1359,  2.0546,  1.4309],\n",
      "        [ 1.0887, -1.0570, -0.6061,  1.2716],\n",
      "        [-1.4751,  0.4576,  0.2822,  1.0001],\n",
      "        [-0.1052, -1.7671,  0.9004, -1.1856]])\n",
      "cell2.weight_hh tensor([[ 1.4010e+00,  1.8883e+00,  6.1574e-01],\n",
      "        [ 9.8430e-01,  2.8774e-01, -5.0935e-01],\n",
      "        [ 1.9035e+00, -1.7805e+00, -3.3360e-01],\n",
      "        [-3.5587e-01,  1.0456e+00,  1.4645e+00],\n",
      "        [ 9.4805e-01,  3.8337e-01,  1.7485e-02],\n",
      "        [ 1.1186e+00, -2.1937e-02, -1.3586e-01],\n",
      "        [ 2.5066e-02, -4.1947e-01,  3.2681e-01],\n",
      "        [ 3.8751e-01, -1.9425e-01, -1.2852e+00],\n",
      "        [ 2.7759e-01,  1.0990e+00, -1.3191e+00],\n",
      "        [ 1.6834e+00, -5.8665e-02, -4.3278e-01],\n",
      "        [-1.6566e-01,  2.7177e-01,  3.7773e-01],\n",
      "        [ 3.7718e-01, -7.0726e-01, -4.5416e-01],\n",
      "        [-1.5876e-03,  5.6869e-02,  2.2038e-02],\n",
      "        [-3.5965e-01, -5.1921e-02,  1.2629e+00],\n",
      "        [ 4.1658e-01,  2.0947e+00, -1.6990e+00],\n",
      "        [-5.4270e-01,  7.4973e-03,  1.5293e+00],\n",
      "        [ 1.4439e+00,  1.9301e+00, -8.8357e-01],\n",
      "        [ 7.1916e-01,  1.6520e+00, -2.3727e-01]])\n",
      "cell2.bias_ih tensor([ 0.2601, -0.9130, -0.9631, -1.6623,  1.9051,  0.9926, -2.1538, -1.3773,\n",
      "        -0.6305, -0.5362, -0.0518, -2.0110,  0.3620, -1.6114,  0.7325, -0.4962,\n",
      "        -0.7712, -0.7449])\n",
      "cell2.bias_hh tensor([-0.5965,  0.8886, -0.6874,  0.8208,  0.4119, -0.3460, -0.5708, -0.2497,\n",
      "        -0.2431,  1.4013,  1.7904, -0.4793,  2.1202, -0.9042, -0.1617, -0.8387,\n",
      "        -0.1808,  0.6643])\n",
      "combine_cell_outputs.weight tensor([[-0.2529,  0.1209,  0.3968,  0.3347,  0.2483,  0.0070],\n",
      "        [ 0.1601, -0.3561,  0.0108,  0.2424,  0.3082, -0.1421],\n",
      "        [ 0.0126,  0.2083, -0.0678,  0.1542,  0.0613,  0.2881]])\n",
      "combine_cell_outputs.bias tensor([-0.0702, -0.3060,  0.4057])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     14\u001b[0m     i\u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss_function(x , y_test)\n\u001b[0;32m     20\u001b[0m     final_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mcombined.forward\u001b[1;34m(self, input, state)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, state):\n\u001b[1;32m----> 9\u001b[0m     hy1, (hy1, cy1) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     hy2, (hy2, cy2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell2\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28minput\u001b[39m, state)\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hy1, hy2))\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[1;34m(self, input, state)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, state: Tuple[Tensor, Tensor, Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tuple[Tensor, Tensor]]:\n\u001b[1;32m---> 13\u001b[0m     hx, cx1, cx2 \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m     15\u001b[0m     gates \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_ih\u001b[38;5;241m.\u001b[39mt()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_ih \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m     16\u001b[0m              torch\u001b[38;5;241m.\u001b[39mmatmul(hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_hh\u001b[38;5;241m.\u001b[39mt()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_hh)\n\u001b[0;32m     18\u001b[0m     ingate1, ingate2, forgetgate1, forgetgate2, cellgate, outgate \u001b[38;5;241m=\u001b[39m gates\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_gates)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "model = combined(input_dim, hidden_dim)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        \n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)  \n",
    "epochs=500\n",
    "final_losses=[]\n",
    "\n",
    "for i in range(epochs):\n",
    "    i= i+1\n",
    "\n",
    "    x = model.forward(input1, (a, b))\n",
    "\n",
    "    loss=loss_function(x , y_test)\n",
    "\n",
    "    final_losses.append(loss)\n",
    "\n",
    "    if i % 100 == 1:\n",
    "\n",
    "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTMCell(1, 16)\n",
    "test_state = (torch.randn(16), torch.randn(16), torch.randn(16))\n",
    "a = (torch.randn(16), torch.randn(16), torch.randn(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [96]\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcy2 \u001b[38;5;241m=\u001b[39m (f2 \u001b[38;5;241m*\u001b[39m cx2) \u001b[38;5;241m+\u001b[39m (i2 \u001b[38;5;241m*\u001b[39m z)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecay_coef \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLC_d(hx)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhy\u001b[49m, (hy, cy1, cy2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hy' is not defined"
     ]
    }
   ],
   "source": [
    "model.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, cell, *cell_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCellUnknownData(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.LC_i1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_i2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_o = Gate_network(input_size, hidden_size)\n",
    "        self.LC_z = Gate_network(input_size, hidden_size)\n",
    "        self.LC_d = Decay_network(input_size, hidden_size)\n",
    "        self.LC_lambda = Decode_layer(input_size, hidden_size)\n",
    "        \n",
    "        self.tj = None\n",
    "        self.cy1 = None\n",
    "        self.cy2 = None\n",
    "        self.decay_coef = None\n",
    "        self.o = None\n",
    "        \n",
    "    def get_intensity(self, t):\n",
    "        h_t = self.get_h(t)\n",
    "        intensity = self.LC_lambda(h_t)\n",
    "        \n",
    "    def get_h(self, t):\n",
    "        c_t = self.calc_c(t)\n",
    "        h_t = self.o * torch.tanh(c_t)\n",
    "        return h_t\n",
    "\n",
    "    def get_c(self, t):\n",
    "        c_t = self.cy2 + (self.cy1 - self.cy2)*torch.exp(-self.decay_coef*(t - tj))\n",
    "        return c_t\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.check_if_tj_is_set()\n",
    "        \n",
    "        hx, cx1, cx2 = state\n",
    "        \n",
    "        i1 = self.LC_i1(hx)\n",
    "        i2 = self.LC_i2(hx)\n",
    "        f1 = self.LC_f1(hx)\n",
    "        f2 = self.LC_f2(hx)\n",
    "        o = self.LC_o(hx)\n",
    "        z = self.Tanh(self.LC_z(hx))\n",
    "\n",
    "        self.o = o\n",
    "        self.cy1 = (f1 * cx1) + (i1 * z)\n",
    "        self.cy2 = (f2 * cx2) + (i2 * z)\n",
    "        self.decay_coef = self.LC_d(hx)\n",
    "        \n",
    "        return self.cy2\n",
    "    \n",
    "    def set_tj(self, tj):\n",
    "        self.tj = tj\n",
    "        \n",
    "    def check_if_tj_is_set(self):\n",
    "        if self.tj == None:\n",
    "            raise Exception('Cannot run forward of LSTM cell before its starting time is set. Use set_tj to set tj.')\n",
    "            \n",
    "class LSTMCellKnownData(nn.Module):\n",
    "    def __init__(self, time_interval, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.LC_i1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_i2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f1 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_f2 = Gate_network(input_size, hidden_size)\n",
    "        self.LC_o = Gate_network(input_size, hidden_size)\n",
    "        self.LC_z = Gate_network(input_size, hidden_size)\n",
    "        self.LC_d = Decay_network(input_size, hidden_size)\n",
    "        self.LC_lambda = Decode_layer(input_size, hidden_size)\n",
    "        \n",
    "        self.t_start, self.t_end = time(interval)\n",
    "        self.cy1 = None\n",
    "        self.cy2 = None\n",
    "        self.decay_coef = None\n",
    "        self.o = None\n",
    "        \n",
    "    def get_intensity(self, h_t):\n",
    "        intensity = self.LC_lambda(h_t)\n",
    "        return intensity\n",
    "        \n",
    "    def get_h(self, c_t):\n",
    "        h_t = self.o * torch.tanh(c_t)\n",
    "        return h_t\n",
    "\n",
    "    def get_c(self):\n",
    "        c_t = self.cy2 + (self.cy1 - self.cy2)*torch.exp(-self.decay_coef*(self.t_end - self.t_start))\n",
    "        return c_t\n",
    "        \n",
    "    def forward(self, state):\n",
    "        self.check_if_tj_is_set()\n",
    "        \n",
    "        hx, cx1, cx2 = state\n",
    "        \n",
    "        i1 = self.LC_i1(hx)\n",
    "        i2 = self.LC_i2(hx)\n",
    "        f1 = self.LC_f1(hx)\n",
    "        f2 = self.LC_f2(hx)\n",
    "        o = self.LC_o(hx)\n",
    "        z = self.Tanh(self.LC_z(hx))\n",
    "\n",
    "        self.o = o\n",
    "        self.cy1 = (f1 * cx1) + (i1 * z)\n",
    "        self.cy2 = (f2 * cx2) + (i2 * z)\n",
    "        self.decay_coef = self.LC_d(hx)\n",
    "        \n",
    "        c_output = self.get_c()\n",
    "        h_output = self.get_h(c_output)\n",
    "        intensity_output = self.get_intensity(h_output)\n",
    "        \n",
    "        state = (h_output, c_output, cy2)\n",
    "        \n",
    "        return intensity_output, state\n",
    "    \n",
    "    def set_tj(self, tj):\n",
    "        self.tj = tj\n",
    "        \n",
    "    def check_if_tj_is_set(self):\n",
    "        if self.tj == None:\n",
    "            raise Exception('Cannot run forward of LSTM cell before its starting time is set. Use set_tj to set tj.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate_network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.Tanh(self.layer1(input))\n",
    "        x = self.Tanh(self.layer2(x))\n",
    "        return self.Sigmoid(x)\n",
    "    \n",
    "class Decay_network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.Softplus(self.layer1(input))\n",
    "        x = self.Softplus(self.layer2(x))\n",
    "        return x\n",
    "    \n",
    "class Decode_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.Softplus(self.layer1(input))\n",
    "        x = self.Softplus(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "class Embed_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Tanh = nn.Tanh\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        x = self.Tanh(self.layer1(input))\n",
    "        x = self.Tanh(self.layer2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_layer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 16):\n",
    "        super().__init__()\n",
    "        self.Softplus = nn.Softplus()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "        self.time_layer = nn.Linear(2, 2)\n",
    "        \n",
    "    def forward(self, input: Tensor, time):\n",
    "        x = self.Softplus(self.layer1(input * time))\n",
    "        x = self.Softplus(self.output_layer(x))\n",
    "        return x\n",
    "    \n",
    "    def get_time(self, t):\n",
    "        time = self.Softplus(self.time_layer(t))\n",
    "        return time\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_framework(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.test1 = Test_layer(2, 3)\n",
    "        self.test2 = Test_layer(2, 3)\n",
    "        \n",
    "    def forward(self, input, time):\n",
    "        x = self.test1(input, time)\n",
    "        time = self.test1.get_time(torch.randn(2))\n",
    "        x = self.test2(x, time)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9269, 0.6812], grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Test_framework(2)\n",
    "model.forward(torch.randn(2), torch.randn(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1.layer1.weight tensor([[-0.3199,  0.6786],\n",
      "        [ 0.1927,  0.3247],\n",
      "        [ 0.5788, -0.6896]])\n",
      "test1.layer1.bias tensor([ 0.1387, -0.2884, -0.3352])\n",
      "test1.output_layer.weight tensor([[ 0.5773, -0.3392, -0.4058],\n",
      "        [-0.5206, -0.1302, -0.3975]])\n",
      "test1.output_layer.bias tensor([-0.0074,  0.0397])\n",
      "test1.time_layer.weight tensor([[ 0.6922,  0.2373],\n",
      "        [ 0.5635, -0.2160]])\n",
      "test1.time_layer.bias tensor([-0.2433, -0.1554])\n",
      "test2.layer1.weight tensor([[ 0.5514, -0.6544],\n",
      "        [-0.3557,  0.0063],\n",
      "        [ 0.5027, -0.0073]])\n",
      "test2.layer1.bias tensor([-0.6543,  0.4370, -0.4045])\n",
      "test2.output_layer.weight tensor([[-0.0213,  0.0353, -0.0183],\n",
      "        [ 0.1831,  0.0750, -0.0770]])\n",
      "test2.output_layer.bias tensor([ 0.4107, -0.1227])\n",
      "test2.time_layer.weight tensor([[ 0.0657, -0.2485],\n",
      "        [-0.0894,  0.5229]])\n",
      "test2.time_layer.bias tensor([-0.2745, -0.3383])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "001b6d5e8c8d1c70c2bf73c4b362256a5cb28fdcf5b48d35800dbe28f3238531"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
